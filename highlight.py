import json
import os
import re

# ğŸ”§ Path builder for chapter
def get_chapter_file_path(book, chapter):
    folder_path = os.path.join("static", "highlights", book)
    os.makedirs(folder_path, exist_ok=True)
    return os.path.join(folder_path, f"{chapter}.json")


# ğŸ“¥ Load highlights
def load_data(book, chapter):
    path = get_chapter_file_path(book, chapter)
    print(f"ğŸ“¥ Loading highlights from: {path}")
    if os.path.exists(path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"âœ… Loaded {len(data)} highlights.")
                return data
        except Exception as e:
            print(f"âŒ Error loading highlights: {e}")
            return []
    else:
        print("âš ï¸ File not found, returning empty list.")
        return []


# ğŸ’¾ Save highlights to file
def save_data(book, chapter, highlights):
    path = get_chapter_file_path(book, chapter)
    try:
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(highlights, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ Saved {len(highlights)} highlights to {path}")
    except Exception as e:
        print(f"âŒ Error saving highlights: {e}")


# ğŸ–ï¸ Save one detected highlight (with metadata)
def save_detected_highlight(book, chapter, text, start, end, category, page_number, match_id=None, rule_name=None, source=None):
    print(f"\nğŸ–ï¸ Saving highlight â†’ Book: {book}, Chapter: {chapter}, Page: {page_number}, Category: {category}")
    highlights = load_data(book, chapter)

    # ğŸš« Skip junk or HTML-like highlights
    if is_junk(text):
        print(f"ğŸš« Skipped junk highlight: '{text}'")
        return

    # âœ… Normalize category
    category = (category or "unknown").strip()

    entry = {
        "text": text.strip(),
        "start": int(start),
        "end": int(end),
        "category": category,
        "page_number": int(page_number),
    }

    if match_id is not None:
        entry["match_id"] = match_id
    if rule_name is not None:
        entry["rule_name"] = rule_name
    if source is not None:
        entry["source"] = source

    if entry not in highlights:
        highlights.append(entry)
        save_data(book, chapter, highlights)
        print(f"âœ… Highlight added: '{text}'")
    else:
        print(f"â„¹ï¸ Highlight already exists: '{text}'")


# ğŸ§½ Remove a highlight
def remove_highlight(book, chapter, text, start, end, category, page_number):
    print(f"\nğŸ§½ Removing highlight â†’ Book: {book}, Chapter: {chapter}, Page: {page_number}, Category: {category}")
    highlights = load_data(book, chapter)

    target = {
        "text": text.strip(),
        "start": int(start),
        "end": int(end),
        "category": category.strip(),
        "page_number": int(page_number)
    }

    new_highlights = [h for h in highlights if not (
        h.get("text") == target["text"] and
        h.get("start") == target["start"] and
        h.get("end") == target["end"] and
        h.get("category") == target["category"] and
        h.get("page_number") == target["page_number"]
    )]

    if len(new_highlights) < len(highlights):
        save_data(book, chapter, new_highlights)
        print("âœ… Highlight removed.")
    else:
        print("âš ï¸ Highlight not found, skipping.")


# ğŸ“Œ Get all highlights (with optional filters)
def get_highlights(book, chapter, page_number=None, category=None):
    print(f"\nğŸ“Œ Fetching highlights â†’ Book: {book}, Chapter: {chapter}, Page: {page_number}, Category: {category}")
    highlights = load_data(book, chapter)

    if page_number is not None:
        page_number = int(page_number)
        highlights = [h for h in highlights if h.get("page_number") == page_number]
        print(f"ğŸ“„ Filtered by page â†’ {len(highlights)} items")

    if category is not None:
        highlights = [h for h in highlights if h.get("category") == category]
        print(f"ğŸ·ï¸ Filtered by category â†’ {len(highlights)} items")

    return highlights


# ğŸš« Junk detector function
def is_junk(text):
    junk_keywords = {
        "html", "head", "body", "div", "class", "span", "style", "script",
        "lang", "href", "meta", "link", "content", "http", "www", "doctype"
    }
    if len(text.strip()) < 3:
        return True
    if any(tag in text.lower() for tag in junk_keywords):
        return True
    if re.match(r'^[\W\d\s]+$', text.strip()):
        return True
    return False
















